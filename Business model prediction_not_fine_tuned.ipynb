{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05b9932",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS THE FIRST ATTEMPT FOR THE BUSINESS PLAN PREDICTION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301cac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, pipeline\n",
    "\n",
    "# Data preprocessing\n",
    "class BusinessPlanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        plan = self.data[idx]\n",
    "        input_ids = self.tokenizer.encode(plan, max_length=self.max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
    "        attention_mask = (input_ids != self.tokenizer.pad_token_id).float()\n",
    "        return input_ids.squeeze(), attention_mask.squeeze()\n",
    "\n",
    "# Model architecture\n",
    "class BusinessPlanGenerator(nn.Module):\n",
    "    def __init__(self, config, input_size):\n",
    "        super().__init__()\n",
    "        self.gpt2 = GPT2LMHeadModel(config)\n",
    "        self.input_projection = nn.Linear(input_size, config.n_embd)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, input_params):\n",
    "        embedded_inputs = self.input_projection(input_params)\n",
    "        outputs = self.gpt2(input_ids, attention_mask=attention_mask, inputs_embeds=embedded_inputs)\n",
    "        return outputs.logits\n",
    "\n",
    "# Training\n",
    "def train(model, dataset, optimizer, device, num_epochs, save_path):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in dataset:\n",
    "            input_ids, attention_mask = batch\n",
    "            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask, input_params)\n",
    "            loss = nn.CrossEntropyLoss()(outputs.view(-1, outputs.size(-1)), input_ids.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "# Generation\n",
    "def generate_business_plan(model, input_params, device, tokenizer, max_length=1024, num_return_sequences=1):\n",
    "    model.eval()\n",
    "    input_ids = torch.zeros(1, 1, dtype=torch.long, device=device)\n",
    "    attention_mask = torch.ones(1, 1, dtype=torch.long, device=device)\n",
    "    input_params = torch.tensor([input_params], dtype=torch.float, device=device)\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        input_params=input_params,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Main\n",
    "if __:\n",
    "    # Load dataset\n",
    "    dataset = BusinessPlanDataset(data, tokenizer, max_length=1024)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Load pre-trained LLM\n",
    "    config = GPT2Config.from_pretrained('gpt2-medium')\n",
    "    model = BusinessPlanGenerator(config, input_size=len(input_params)).to(device)\n",
    "    model.gpt2.load_state_dict(torch.load('gpt2-medium.pth'))\n",
    "\n",
    "    # Fine-tune the model\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    train(model, dataloader, optimizer, device, num_epochs=10, save_path='business-plan-generator.pth')\n",
    "\n",
    "    # Generate a business plan\n",
    "    input_params = [10, 15, 20, 1000, 500]\n",
    "    business_plan = generate_business_plan(model, input_params, device, tokenizer)\n",
    "    print(business_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc5f9fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING CHATGPT PROMPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c800932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, pipeline\n",
    "\n",
    "# Data preprocessing\n",
    "class BusinessPlanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        crop_allocation, budget, land_area = self.data[idx]\n",
    "        input_params = torch.tensor([crop_allocation, budget, land_area], dtype=torch.float)\n",
    "        return input_params\n",
    "\n",
    "# Model architecture\n",
    "class BusinessPlanGenerator(nn.Module):\n",
    "    def __init__(self, config, input_size):\n",
    "        super().__init__()\n",
    "        self.gpt2 = GPT2LMHeadModel(config)\n",
    "        self.input_projection = nn.Linear(input_size, config.n_embd)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, input_params):\n",
    "        embedded_inputs = self.input_projection(input_params)\n",
    "        outputs = self.gpt2(input_ids, attention_mask=attention_mask, inputs_embeds=embedded_inputs)\n",
    "        return outputs.logits\n",
    "\n",
    "# Training\n",
    "def train(model, dataset, optimizer, device, num_epochs, save_path):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in dataset:\n",
    "            input_params = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(None, None, input_params)\n",
    "            loss = nn.MSELoss()(outputs, input_params)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "# Generation\n",
    "def generate_business_plan(model, input_params, device, tokenizer, llm, max_length=1024, num_return_sequences=1):\n",
    "    model.eval()\n",
    "    input_params = torch.tensor([input_params], dtype=torch.float, device=device)\n",
    "    outputs = model(None, None, input_params)\n",
    "    \n",
    "    prompt = f\"Based on the crop allocation of {input_params[0].item()}, {input_params[1].item()}, and {input_params[2].item()} square meters, a budget of ${input_params[1].item()}, and a total land area of {input_params[2].item()} square meters, generate a detailed business plan.\"\n",
    "    business_plan = llm(prompt, max_length=max_length, num_return_sequences=num_return_sequences, do_sample=True, top_k=50, top_p=0.95, num_beams=5, early_stopping=True)[0]['generated_text']\n",
    "    \n",
    "    return business_plan\n",
    "\n",
    "# Main\n",
    "if __:\n",
    "    # Load dataset\n",
    "    dataset = BusinessPlanDataset(data, tokenizer, max_length=1024)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Load pre-trained LLM\n",
    "    config = GPT2Config.from_pretrained('gpt2-medium')\n",
    "    model = BusinessPlanGenerator(config, input_size=3).to(device)\n",
    "    model.gpt2.load_state_dict(torch.load('gpt2-medium.pth'))\n",
    "\n",
    "    # Fine-tune the model\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    train(model, dataloader, optimizer, device, num_epochs=10, save_path='business-plan-generator.pth')\n",
    "\n",
    "    # Generate a business plan\n",
    "    input_params = [10, 15, 20, 1000, 500]\n",
    "    llm = pipeline('text-generation', model='gpt2')\n",
    "    business_plan = generate_business_plan(model, input_params, device, tokenizer, llm)\n",
    "    print(business_plan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
